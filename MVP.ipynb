{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'574 successes, 0 failures'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Collection\n",
    "\n",
    "import requests, os, re, json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "progress = display(display_id=True)\n",
    "successes, failures = 0, 0\n",
    "visited = set()\n",
    "to_visit = set()\n",
    "dataset = []\n",
    "\n",
    "DOMAIN = \"https://charlotte.edu\"\n",
    "EXCLUDE = ['?', 'page', 'gateway', 'illiad', 'news-articles', 'news-events', 'news-media', 'linkedin', 'facebook', 'twitter', 'instagram', 'youtube', 'flickr', 'pinterest', '.com', '.org', '.net', '.gov', '.pdf', '.doc', 'xml', 'php', 'mailto:', '@', 'tel:', 'javascript:', 'tel:', 'sms:', 'mailto:', 'angular', 'react', '.js', 'event', 'corporate', '#', 'image', 'gallery', 'taskstream-student-handbook']    \n",
    "WORKERS = 50\n",
    "\n",
    "def recursive_scrape(domain):\n",
    "    global successes, failures, visited, to_visit, dataset\n",
    "    if domain in visited:\n",
    "        return\n",
    "    visited.add(domain)\n",
    "    try:\n",
    "        response = requests.get(domain)\n",
    "        if response.status_code != 200:\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        get_urls(soup)\n",
    "        extract_data(soup, domain)\n",
    "    except Exception as e:\n",
    "        print(f'Error {e}')\n",
    "        failures += 1\n",
    "    successes += 1\n",
    "        \n",
    "    progress.update(f'{successes} successes, {failures} failures')\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "        executor.map(recursive_scrape, to_visit)\n",
    "        \n",
    "def get_urls(soup):\n",
    "    global visited, to_visit\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        if any(exclude in href for exclude in EXCLUDE):\n",
    "            continue\n",
    "        if href.startswith('/'):\n",
    "            href = urljoin(DOMAIN, href)\n",
    "        if (\"charlotte.edu\" in href) and (href not in visited) and (href not in to_visit):\n",
    "            to_visit.add(href)\n",
    "\n",
    "def extract_data(soup, url):\n",
    "    global dataset\n",
    "    title = soup.title.string if soup.title else ''\n",
    "    \n",
    "    # Find the \"main\", \"main-content\", or \"body\" element\n",
    "    element_ids = [\"main\", \"main-content\", \"body\"]\n",
    "    element = None\n",
    "\n",
    "    for elem_id in element_ids:\n",
    "        element = soup.find(id=elem_id)\n",
    "        if element:\n",
    "            if elem_id == \"main-content\":\n",
    "                element = element.parent\n",
    "            break\n",
    "\n",
    "    # Extract all visible text in the element and its child elements\n",
    "    text = element.get_text(strip=True, separator=' ') if element else ''\n",
    "\n",
    "    # Clean up the text\n",
    "    text = text.replace('\"', \"'\")    \n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('\\t', '')    \n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    dataset.append({'url': url, 'title': title, 'text': text})\n",
    "\n",
    "recursive_scrape(DOMAIN)\n",
    "\n",
    "with open('./data/dataset.json', 'w') as f:\n",
    "    f.write(json.dumps(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import sys\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import clear_output, display\n",
    "import csv\n",
    "csv.field_size_limit(2**31 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = './data/endpoints.csv'\n",
    "DOMAIN = 'https://charlotte.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url parameters to exclude/skip\n",
    "EXCLUDE = [\n",
    "    'news-articles',\n",
    "    'news-events',\n",
    "    'news-media',\n",
    "    'linkedin',\n",
    "    'facebook',\n",
    "    'twitter',\n",
    "    'instagram',\n",
    "    'youtube',\n",
    "    'flickr',\n",
    "    'pinterest',\n",
    "    '.com',\n",
    "    '.org',\n",
    "    '.net',\n",
    "    '.gov',\n",
    "    '.pdf',\n",
    "    '.doc',\n",
    "    'xml',\n",
    "    'php',\n",
    "    'mailto:',\n",
    "    '@',\n",
    "    'tel:',\n",
    "    'javascript:',\n",
    "    'tel:',\n",
    "    'sms:',\n",
    "    'mailto:',\n",
    "    'angular',\n",
    "    'react',\n",
    "    '.js',\n",
    "    'event',\n",
    "    'corporate',\n",
    "    '#',\n",
    "    'image',\n",
    "    'gallery',\n",
    "    'taskstream-student-handbook',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most recent URL: https://sites.charlotte.edu/harwood/?sfid=9267&sf_paged=24 \\nStatus: Success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Successes: 14897, Failures: 492'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "# Display the progress of the web scraping\n",
    "current_display = display('Starting...', display_id=True)\n",
    "progress_display = display('Starting...', display_id=True)\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "\n",
    "def print_status(url, status):\n",
    "    global success_count\n",
    "    global failure_count\n",
    "    global current_display\n",
    "    global progress_display\n",
    "    \n",
    "    if status == 'Success':\n",
    "        success_count += 1\n",
    "    if status == 'Failed':\n",
    "        failure_count += 1\n",
    "    if status == 'Exception':\n",
    "        failure_count += 1\n",
    "        \n",
    "    # Print the most recent URL and status, and the total counts    \n",
    "    current_display.update(f'Most recent URL: {url} \\nStatus: {status}')\n",
    "    progress_display.update(f'Successes: {success_count}, Failures: {failure_count}')\n",
    "    \n",
    "\n",
    "def remove_url_prefix(url):\n",
    "    url = url.replace('http://', '').replace('https://', '').replace('www.', '')\n",
    "    return url.lower()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    if any(ex in url for ex in EXCLUDE) or len(url) < 8 or len(url) > 100:\n",
    "        return False\n",
    "    try:\n",
    "        split_url = re.split('https?://', url)\n",
    "        return 'mailto:' not in url and '@' not in url and 'charlotte.edu' in split_url[0] or len(split_url) > 1 and 'charlotte.edu' in split_url[1]\n",
    "    except Exception as e:\n",
    "        print_status(url, 'Exception')\n",
    "        return False\n",
    "    \n",
    "def write_to_csv(valid_endpoints, failed_endpoints):\n",
    "    # Export the endpoints to a CSV file\n",
    "    try:\n",
    "        with open(FILENAME, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['URL', 'Text'])  # Write the column labels\n",
    "            for endpoint in valid_endpoints:  # Write the valid endpoints\n",
    "                writer.writerow(endpoint)\n",
    "            # for endpoint in failed_endpoints:  # Write the failed endpoints\n",
    "            #     writer.writerow(endpoint)\n",
    "            file.flush()\n",
    "            os.fsync(file.fileno())\n",
    "    except Exception as e:\n",
    "        print_status(None, 'Failed')\n",
    "\n",
    "def fetch_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return ('Success', response.content)\n",
    "        else:\n",
    "            return ('Failed', None)\n",
    "    except (requests.exceptions.RequestException, requests.exceptions.Timeout, ValueError):\n",
    "        print_status(url, 'Failed')\n",
    "        return ('Failed', None)\n",
    "def crawl_domain(domain):\n",
    "    # Initialize sets and lists to keep track of visited URLs, URLs to visit, and endpoints\n",
    "    visited = set()\n",
    "    to_visit = [domain.rstrip('/')]\n",
    "    valid_endpoints = []\n",
    "    failed_endpoints = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the web scraping\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        \n",
    "        # Submit tasks to the executor for each URL in to_visit that hasn't been visited yet and is valid\n",
    "        futures = {executor.submit(fetch_url, url): url for url in to_visit if is_valid_url(url) and url not in visited}\n",
    "        # Add the URLs that are being visited to the visited set\n",
    "        visited.update(url for url in to_visit if is_valid_url(url))\n",
    "\n",
    "        # Continue until all futures are done\n",
    "        while futures:\n",
    "            # Wait for the first future to complete\n",
    "            done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "\n",
    "            # Process each completed future\n",
    "            for future in done:\n",
    "                url = futures.pop(future)\n",
    "\n",
    "                try:\n",
    "                    # Get the result of the future\n",
    "                    data = future.result()\n",
    "                except Exception as e:\n",
    "                    # If an exception occurred while fetching the URL, print the status and continue\n",
    "                    print_status(url, 'Failed')\n",
    "                    continue\n",
    "                \n",
    "                # Unpack the status and content from the data\n",
    "                status, content = data\n",
    "                \n",
    "                try:\n",
    "                    if status == 'Success':\n",
    "                        # Parse the HTML content\n",
    "                        soup = BeautifulSoup(content, 'html.parser')\n",
    "                        \n",
    "                        # Link extraction\n",
    "                        # Find all links in the HTML content\n",
    "                        links = soup.find_all('a')\n",
    "                        for link in links:\n",
    "                            href = link.get('href')\n",
    "                            if href is not None:\n",
    "                                # Resolve relative links to absolute links\n",
    "                                full_url = urljoin(domain, href).rstrip('/')\n",
    "                                clean_url = remove_url_prefix(full_url)\n",
    "                                slash_count = urlparse(clean_url).path.count('/')\n",
    "                                # If the URL is valid, hasn't been visited yet, and doesn't have too many slashes, add it to the futures\n",
    "                                if is_valid_url(clean_url) and slash_count <= 2 and clean_url not in visited:\n",
    "                                    futures[executor.submit(fetch_url, full_url)] = full_url\n",
    "                                    visited.add(clean_url)\n",
    "                        \n",
    "                        # Text extraction\n",
    "                        # Remove JavaScript and CSS blocks, and common sections like headers, footers, etc.\n",
    "                        for script in soup([\"script\", \"style\"]):  # Remove JavaScript and CSS blocks\n",
    "                            script.decompose()\n",
    "                        # Remove header, footer, nav, and aside\n",
    "                        for tag in soup([\"header\", \"footer\", \"nav\"]):\n",
    "                            tag.decompose() \n",
    "                        # Remove divs with class \"sidebar\" or \"ad\"\n",
    "                        for div in soup.find_all(\"div\", class_=[\"sidebar\", \"ad\"]):\n",
    "                            div.decompose()\n",
    "                        # Extract text from the HTML content\n",
    "                        text = soup.get_text()  \n",
    "                        text = ' '.join(text.split())\n",
    "                        # Save the text along with the URL and status\n",
    "                        valid_endpoints.append([url, str(text)])  \n",
    "                        print_status(url, status)\n",
    "                        \n",
    "                    else:\n",
    "                        # If the status is not 'Success', add the URL and status to the failed_endpoints list\n",
    "                        failed_endpoints.append([url, ''])\n",
    "                        visited.add(url)\n",
    "                except Exception as e:\n",
    "                    # If an exception occurred while processing the HTML content, print the status and continue\n",
    "                    print_status(url, 'Failed')\n",
    "                    visited.add(url)\n",
    "                    continue\n",
    "        # Write the valid and failed endpoints to a CSV file\n",
    "        write_to_csv(valid_endpoints, failed_endpoints)\n",
    "        \n",
    "# Clear the csv file\n",
    "open(FILENAME, 'w').close()\n",
    "\n",
    "# Crawl the domain\n",
    "crawl_domain(DOMAIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

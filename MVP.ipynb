{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEVELOPING A SEMANTIC SEARCH ENGINE USING NATURAL LANGUAGE PROCESSING AND MACHINE LEARNING\n",
    "\n",
    "University of North Carolina at Charlotte\n",
    "\n",
    "ITCS 6150 - Intelligent Systems, Spring 2024\n",
    "\n",
    "Ryan Hull, \n",
    "Albert Oh, \n",
    "Tim Hillmann, \n",
    "Adam Lowder\n",
    "\n",
    "## Introduction\n",
    "The University of North Carolina at Charlotte's website hosts a vast array of information, yet finding specific content can be challenging using traditional search methods. Our project aims to introduce a semantic search engine that utilizes machine learning and natural language processing to understand and interpret user queries contextually, aiming for significantly improved accuracy and relevance in search results. \n",
    "\n",
    "## Problem Statement \n",
    "Our objective is to develop a search engine that enhances traditional keyword matching by employing machine learning algorithms to analyze and understand the context and intent behind user queries. The challenge lies in effectively processing and interpreting the expansive and diverse content on the university's website, requiring a holistic approach to data preprocessing, semantic analysis, and embedding generation in an ML pipeline.\n",
    "\n",
    "## Table of Contents \n",
    "- [Introduction](#introduction)\n",
    "- [Problem Statement](#problem-statement)\n",
    "- [Methodology](#methodology)  \n",
    "- [Pipeline](#pipeline)\n",
    "    - [Data Collection](#data-collection)\n",
    "    - [Data Preprocessing](#data-preprocessing)\n",
    "    - [Summarization](#summarization)\n",
    "    - [Semantic Analysis/Embedding](#semantic-analysis)\n",
    "    - [Search Engine](#search-engine)\n",
    "    - [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "Our methodology begins with recursive/deep crawling the Universityâ€™s website, preprocessing the site data, including the extraction and cleaning of text. Following this, we will generate context aware summaries using a pre-trained LLM model, then create embeddings in a similar manner to capture the semantic meaning of texts. For the search engine's development, we plan to embed user queries and perform similarity searches on our embeddings database, utilizing a small custom front-end web application for communication to the user. Finally, we will evaluate the search engine's performance with A-B testing against the UNC Charlotte website.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Over the course of this project we explored several different ways to traverse and collect website data while utilizing Python libraries such as beautifulsoup and Playwright. \n",
    "\n",
    "Using a recursive web crawler approach, we were able to traverse the entire website and collect all the text data from the website. This resulting in an large dataset >4GB of text data alone (later skimmed down to ~47mb), and over 14,000 sites. While this worked, we found that the data was too large to process in a reasonable amount of time, as well as included various sources of outdated information that were not ideal or wanted in our project. In the end we have two datasets, one that is a subset of the entire website with stop-words removed and limits to path count (allowing for filtering by path count or subdomain) called endpoints.csv and another manually picked test set which we will demonstrate in this pipeline, called subdomain_test_set.csv. Note that the text data in these datasets is not the full text data, but rather a subset of the text data that was collected from the website and cleaned. However, no manual cleaning was done to the text data.\n",
    "\n",
    "#### How we chose our test set\n",
    "\n",
    "The test we will demonstrate includes pages from the main website, as well as any base path for subdomains that we found. The length of the pages ranged from a few paragraphs to a few pages of text. We chose these pages because they were the most relevant to the main website and would be the most likely to be searched for, as well as show a diverse range of text data and topics.\n",
    "\n",
    "#### Technical Methods\n",
    "\n",
    "* The main libraries used here are `requests`, `concurrent.futures`, and `BeautifulSoup`, in order to crawl a domain, extract links and text from the HTML content of each page, and write the results to a CSV file. We also included some error handling and progress reporting.\n",
    "\n",
    "1. **Parallelization**: Our method uses `concurrent.futures.ThreadPoolExecutor` to fetch multiple URLs at the same time. This can significantly speed up the web scraping, but it also adds complexity. In practice we found that for small datasets it is not necessary to use parallelization, but for larger datasets it increases speed linearlly.\n",
    "\n",
    "2. **Error handling**: The code includes several `try`/`except` blocks to handle exceptions that might occur when fetching a URL or processing the HTML content.\n",
    "\n",
    "3. **Progress reporting**: We display the progress of the web scraping, including the most recent URL and status, and the total number of successes and failures, making it useful to see progress in long running tasks.\n",
    "\n",
    "4. **Link and text extraction**: The code includes logic to extract links and text from the HTML content, resolve relative links to absolute links, and remove certain sections of the page (like JavaScript and CSS blocks, headers, footers, etc.). These were improved upon by trial and error, and may not be perfect, but they work well for the most part and can be improved upon.\n",
    "\n",
    "5. **URL validation and filtering**: We include logic to validate URLs and filter out certain URLs based on a list of excluded terms. This is necessary to avoid crawling irrelevant or unwanted pages.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "* The code does not handle JavaScript-heavy websites well, and may not be able to extract all the text from such websites. This is because the code only processes the HTML content of a page, and does not execute JavaScript. This is a limitation of the `requests` and `BeautifulSoup` libraries, and would require a more complex solution to solve such as using a headless browser like Playwright or Selenium. This switch would also allow for more complex interactions with the website, such as clicking buttons or filling out forms, which would be useful for more complex websites, however it would also slow down the process significantly.\n",
    "\n",
    "* This is currently only useful for public sites only, as it does not handle authentication or cookies. \n",
    "\n",
    "#### Future Improvements\n",
    "\n",
    "* We could improve the code to handle JavaScript-heavy websites by using a headless browser like Playwright or Selenium. This would allow us to execute JavaScript and extract the text from the rendered page.\n",
    "\n",
    "* Currently we store the page data for every visited page, and later process and filter out the data set we want to be able to search over. This limits our calls to the site, however is not ideal. If we define the filters we want during the preprocessing step, we could avoid storing the data we don't want in the first place. Traversal of the entire site would still be necessary, but we could avoid storing the data we don't want.\n",
    "\n",
    "* Additionally, storing the entirely html could allow for comparing the hash of the html to see if the page has changed, and only storing the new data / sending it through the pipeline. This would allow for a more up to date dataset and search engine, and would allow for the dataset to be updated over time in a more efficient manner. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most recent URL: https://eng-resources.charlotte.edu/unccengkit/cooling, Status: Success, Exception: None'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Successes: 14665, Failures: 167'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "c:\\Python312\\Lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "# Data Collection\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import requests \n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "csv.field_size_limit(2**31 - 1)\n",
    "\n",
    "RUN = True # Set to False to skip the web scraping, and use the existing CSV file\n",
    "\n",
    "FILENAME = './data/endpoints.csv' # The file to write the endpoints to\n",
    "DOMAIN = 'https://charlotte.edu' # The domain to crawl\n",
    "PATH_LIMIT = 2  # Maximum number of slashes in the URL path\n",
    "\n",
    "# Url parameters to exclude/skip\n",
    "EXCLUDE = ['?', 'page', 'gateway', 'illiad', 'news-articles', 'news-events', 'news-media', 'linkedin', 'facebook', 'twitter', 'instagram', 'youtube', 'flickr', 'pinterest', '.com', '.org', '.net', '.gov', '.pdf', '.doc', 'xml', 'php', 'mailto:', '@', 'tel:', 'javascript:', 'tel:', 'sms:', 'mailto:', 'angular', 'react', '.js', 'event', 'corporate', '#', 'image', 'gallery', 'taskstream-student-handbook']    \n",
    "\n",
    "# Display the progress of the web scraping\n",
    "current_display = display('Starting...', display_id=True)\n",
    "progress_display = display('Starting...', display_id=True)\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "\n",
    "def print_status(url, status, exception=None):\n",
    "    global success_count\n",
    "    global failure_count\n",
    "    global current_display\n",
    "    global progress_display\n",
    "    \n",
    "    if status == 'Success':\n",
    "        success_count += 1\n",
    "    if status == 'Failed':\n",
    "        failure_count += 1\n",
    "    if status == 'Exception':\n",
    "        failure_count += 1\n",
    "        \n",
    "    # Print the most recent URL and status, and the total counts    \n",
    "    current_display.update(f'Most recent URL: {url}, Status: {status}, Exception: {exception}')\n",
    "    progress_display.update(f'Successes: {success_count}, Failures: {failure_count}')\n",
    "\n",
    "def remove_url_prefix(url):\n",
    "    url = url.replace('http://', '').replace('https://', '').replace('www.', '')\n",
    "    return url.lower()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    if any(ex in url for ex in EXCLUDE) or len(url) < 8 or len(url) > 100:\n",
    "        return False\n",
    "    try:\n",
    "        split_url = re.split('https?://', url)\n",
    "        return 'mailto:' not in url and '@' not in url and 'charlotte.edu' in split_url[0] or len(split_url) > 1 and 'charlotte.edu' in split_url[1]\n",
    "    except Exception as e:\n",
    "        print_status(url, 'Exception', e)\n",
    "        return False\n",
    "    \n",
    "def write_to_csv(valid_endpoints, failed_endpoints):\n",
    "    # Export the endpoints to a CSV file\n",
    "    try:\n",
    "        with open(FILENAME, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['URL', 'Title', 'Text'])  # Write the column labels\n",
    "            for endpoint in valid_endpoints:  # Write the valid endpoints\n",
    "                writer.writerow(endpoint)\n",
    "            # for endpoint in failed_endpoints:  # Write the failed endpoints\n",
    "            #     writer.writerow(endpoint)\n",
    "            file.flush()\n",
    "            os.fsync(file.fileno())\n",
    "    except Exception as e:\n",
    "        print_status(None, 'Failed', e)\n",
    "\n",
    "def fetch_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return ('Success', response.content)\n",
    "        else:\n",
    "            return ('Failed', None)\n",
    "    except (requests.exceptions.RequestException, requests.exceptions.Timeout, ValueError):\n",
    "        print_status(url, 'Failed')\n",
    "        return ('Failed', None)\n",
    "    \n",
    "def crawl_domain(domain):\n",
    "    # Initialize sets and lists to keep track of visited URLs, URLs to visit, and endpoints\n",
    "    visited = set()\n",
    "    to_visit = [domain.rstrip('/')]\n",
    "    valid_endpoints = []\n",
    "    failed_endpoints = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the web scraping\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        \n",
    "        # Submit tasks to the executor for each URL in to_visit that hasn't been visited yet and is valid\n",
    "        futures = {executor.submit(fetch_url, url): url for url in to_visit if is_valid_url(url) and url not in visited}\n",
    "        # Add the URLs that are being visited to the visited set\n",
    "        visited.update(url for url in to_visit if is_valid_url(url))\n",
    "\n",
    "        # Continue until all futures are done\n",
    "        while futures:\n",
    "            # Wait for the first future to complete\n",
    "            done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "\n",
    "            # Process each completed future\n",
    "            for future in done:\n",
    "                url = futures.pop(future)\n",
    "\n",
    "                try:\n",
    "                    # Get the result of the future\n",
    "                    data = future.result()\n",
    "                except Exception as e:\n",
    "                    # If an exception occurred while fetching the URL, print the status and continue\n",
    "                    print_status(url, 'Failed')\n",
    "                    continue\n",
    "                \n",
    "                # Unpack the status and content from the data\n",
    "                status, content = data\n",
    "                \n",
    "                try:\n",
    "                    if status == 'Success':\n",
    "                        # Parse the HTML content\n",
    "                        soup = BeautifulSoup(content, 'html.parser')\n",
    "                        \n",
    "                        # Link extraction\n",
    "                        # Find all links in the HTML content\n",
    "                        links = soup.find_all('a')\n",
    "                        for link in links:\n",
    "                            href = link.get('href')\n",
    "                            if href is not None:\n",
    "                                # Resolve relative links to absolute links\n",
    "                                full_url = urljoin(domain, href).rstrip('/')\n",
    "                                clean_url = remove_url_prefix(full_url)\n",
    "                                slash_count = urlparse(clean_url).path.count('/')\n",
    "                                # If the URL is valid, hasn't been visited yet, and doesn't have too many slashes, add it to the futures\n",
    "                                if is_valid_url(clean_url) and slash_count <= PATH_LIMIT and clean_url not in visited:\n",
    "                                    futures[executor.submit(fetch_url, full_url)] = full_url\n",
    "                                    visited.add(clean_url)\n",
    "                        \n",
    "                        # Text extraction\n",
    "                        \n",
    "                        # Save title if it exists\n",
    "                        title_text = soup.title.string if soup.title else ''\n",
    "                        \n",
    "                        # Find the \"main\", \"main-content\", or \"body\" element\n",
    "                        element = None\n",
    "                        main_element = soup.find(id=\"main\")\n",
    "                        main_content_element = soup.find(id=\"main-content\")\n",
    "\n",
    "                        if main_element is not None:\n",
    "                            element = main_element\n",
    "                        elif main_content_element is not None:\n",
    "                            element = main_content_element.parent\n",
    "                        else:\n",
    "                            element = soup.find('body')\n",
    "\n",
    "                        # Extract all visible text in the element and its child elements\n",
    "                        if element:\n",
    "                            text = element.get_text(strip=True, separator=' ')\n",
    "                        else:\n",
    "                            text = ''\n",
    "\n",
    "                        text = text.replace('\"', \"'\")  # Replace all double quotes with single quotes\n",
    "                        text = text.replace('\\n', '')  # Remove new lines\n",
    "                        text = text.replace('\\t', '')  # Remove tabs\n",
    "                        text = ' '.join(text.split())\n",
    "                        # Save the text along with the URL and status\n",
    "                        valid_endpoints.append([str(url), str(title_text), str(text)])  \n",
    "                        print_status(url, status)\n",
    "                        if len(valid_endpoints) % 100 == 0:\n",
    "                            write_to_csv(valid_endpoints, failed_endpoints)\n",
    "                        \n",
    "                    else:\n",
    "                        # If the status is not 'Success', add the URL and status to the failed_endpoints list\n",
    "                        failed_endpoints.append([url, ''])\n",
    "                        visited.add(url)\n",
    "                except Exception as e:\n",
    "                    # If an exception occurred while processing the HTML content, print the status and continue\n",
    "                    print_status(url, 'Failed: ' + str(e))\n",
    "                    visited.add(url)\n",
    "                    continue\n",
    "        # Write the valid and failed endpoints to a CSV file\n",
    "        write_to_csv(valid_endpoints, failed_endpoints)\n",
    "\n",
    "\n",
    "if not RUN: # If RUN is False, skip the web scraping and use the existing CSV file\n",
    "    print('Skipping web scraping...')\n",
    "    \n",
    "else: # If RUN is True, perform the web scraping    \n",
    "    os.makedirs(os.path.dirname(FILENAME), exist_ok=True) # Create the directory if it doesn't exist\n",
    "    open(FILENAME, 'w').close() # Clear the csv file\n",
    "    crawl_domain(DOMAIN) # Crawl the domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "In the data preprocessing phase of our project, we employed Python's powerful pandas library to refine and transform our raw dataset into a structured format suitable for further analysis and usage in our semantic search engine. The process involved several crucial steps, each aimed at improving the quality and relevance of the data extracted during the collection phase.\n",
    "\n",
    "#### Initial Data Cleaning\n",
    "The first step involved importing the raw dataset from a CSV file and removing any rows where essential information was missing. Specifically, we dropped entries without valid 'URL' or 'Text' fields to ensure that only complete records were processed further, as we;; as. This initial cleaning helps in reducing noise and focusing on meaningful data.\n",
    "\n",
    "#### Filtering URLs\n",
    "We applied filters to remove URLs containing specific terms that were deemed irrelevant or unwanted for our project's scope. By applying a custom lambda function, we could efficiently exclude URLs based on a defined list of terms, thereby narrowing down our dataset to include only the content relevant to our objectives. In this case, we filtered out URLs containing `?`, since these are often dynamic pages containing search parameters.\n",
    "\n",
    "#### Sorting and Filtering by Structural Attributes\n",
    "To better understand the website's architecture and prioritize content, we sorted the data based on the URL path count and text length. Entries were ordered by their path complexity to identify and segregate main pages from deeper, potentially less relevant subpages. Additionally, we filtered out text data that fell below a certain length threshold, focusing on substantial content that is more likely to be of interest to users.\n",
    "\n",
    "#### Deduplication and Final Arrangement\n",
    "Duplicate URLs were removed to avoid redundant processing and to ensure the uniqueness of each data point in our dataset. This deduplication step is critical for maintaining a clean and efficient search index. Furthermore, we sorted the remaining entries based on URL length to prioritize shorter, typically more significant URLs.\n",
    "\n",
    "#### Data Conversion and Export\n",
    "The final step in our preprocessing involved converting the cleaned DataFrame back into a list format for compatibility with subsequent processing stages and exporting the refined dataset back into CSV format for easy access and use in future tasks. This step marks the transition from raw to processed data, ready for integration into our semantic search pipeline.\n",
    "\n",
    "#### Limitations\n",
    "While our preprocessing methods have significantly improved dataset usability, they are primarily designed for structured, text-based content and might not handle non-textual data or highly dynamic content effectively. Moreover, the manual selection of filter terms and length thresholds requires domain knowledge and might not capture all nuances of the dataset.\n",
    "\n",
    "#### Future Improvements\n",
    "In future iterations, we aim to enhance our preprocessing pipeline by incorporating natural language processing techniques for better text analysis and by developing more sophisticated criteria for URL filtering and text relevance assessment. Automating the selection of filter terms and thresholds based on data characteristics and user feedback could also enhance the adaptability and effectiveness of our preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>path_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://charlotte.edu</td>\n",
       "      <td>The University of North Carolina at Charlotte ...</td>\n",
       "      <td>UNC Charlotte Icons 0 percent of new undergrad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>http://hi.charlotte.edu</td>\n",
       "      <td>Health Informatics and Analytics</td>\n",
       "      <td>Master of Health Informatics and Analytics Hea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>http://lc.charlotte.edu</td>\n",
       "      <td>Learning Communities</td>\n",
       "      <td>Homepage apply online now! UNC Charlotteâ€™s Lea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>http://faq.charlotte.edu</td>\n",
       "      <td>\\r\\n\\tKnowledge Base\\r\\n</td>\n",
       "      <td>Updating... Skip to main content Filter your s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>http://bcp.charlotte.edu</td>\n",
       "      <td>Emergency Management</td>\n",
       "      <td>Home The University is under normal operations...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           URL  \\\n",
       "0        https://charlotte.edu   \n",
       "3725   http://hi.charlotte.edu   \n",
       "4738   http://lc.charlotte.edu   \n",
       "4239  http://faq.charlotte.edu   \n",
       "4813  http://bcp.charlotte.edu   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     The University of North Carolina at Charlotte ...   \n",
       "3725                   Health Informatics and Analytics   \n",
       "4738                               Learning Communities   \n",
       "4239                           \\r\\n\\tKnowledge Base\\r\\n   \n",
       "4813                               Emergency Management   \n",
       "\n",
       "                                                   Text  path_count  \n",
       "0     UNC Charlotte Icons 0 percent of new undergrad...           0  \n",
       "3725  Master of Health Informatics and Analytics Hea...           0  \n",
       "4738  Homepage apply online now! UNC Charlotteâ€™s Lea...           0  \n",
       "4239  Updating... Skip to main content Filter your s...           0  \n",
       "4813  Home The University is under normal operations...           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def import_and_clean_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset=['URL', 'Title', 'Text'])\n",
    "    return df\n",
    "\n",
    "def filter_urls(df, filter_out_terms):\n",
    "    mask = df['URL'].apply(lambda x: any(term in x for term in filter_out_terms))\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def sort_by_path_count(df):\n",
    "    df['path_count'] = df['URL'].apply(lambda url: url.count('/') - 2)\n",
    "    df = df.sort_values(by=['path_count'])\n",
    "    return df\n",
    "\n",
    "def filter_by_text_length(df, min_length):\n",
    "    df.loc[:, 'Text'] = df['Text'].astype(str)\n",
    "    df = df[df['Text'].str.len() > min_length]\n",
    "    return df\n",
    "\n",
    "def sort_by_url_length(df):\n",
    "    df['URL_length'] = df['URL'].str.len()\n",
    "    df = df.sort_values(by=['URL_length'])\n",
    "    df = df.drop(columns=['URL_length'])\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates(subset=['URL'])\n",
    "    return df\n",
    "\n",
    "def save_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def convert_to_list(df):\n",
    "    return list(df.itertuples(index=False, name=None))\n",
    "\n",
    "# Usage\n",
    "endpoints = import_and_clean_data('./data/endpoints.csv')\n",
    "endpoints = filter_urls(endpoints, ['?'])\n",
    "endpoints = sort_by_path_count(endpoints)\n",
    "subdomain_list = endpoints[endpoints['path_count'] == 0] # Subdomain test set\n",
    "subdomain_list = filter_by_text_length(subdomain_list, 50)\n",
    "subdomain_list = sort_by_url_length(subdomain_list)\n",
    "subdomain_list = remove_duplicates(subdomain_list)\n",
    "save_to_csv(subdomain_list, './data/subdomain_test_set.csv')\n",
    "test_set = convert_to_list(subdomain_list)\n",
    "subdomain_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "In the summarization phase of our project, we utilized a pre-trained language model to generate context-aware summaries of the text data extracted from the University's website. The goal of this phase was to distill the content of each page into concise, informative summaries that capture the essence of the original text and facilitate semantic analysis and embedding generation. An important focus was to ensure the url path and subdomain added context to the summary.\n",
    "\n",
    "#### Model Selection \n",
    "There are many pre-trained language models available, and it is an ever improving and growing list. For our project we wanted an open-source, instruct based, and smaller sized LLM that could run reliably on Google Colab and 8gb GPUs. In the end we chose [Google's Gemma-2b-it model](https://huggingface.co/google/gemma-2b-it), as through our initial compairson and research it provided a good balance of performance and size for our needs.\n",
    "\n",
    "#### Environment Setup\n",
    "We initiated our summarization process by setting up the necessary environment and determining whether the process was running on Google Colab to access specific resources and manage authentication securely. This setup included configuring our model and tokenizer with appropriate parameters for optimal performance across local and cloud-based environments.\n",
    "\n",
    "#### Summary Generation Mechanism\n",
    "Our approach employed a structured instructive prompt tailored to guide the LLM in producing summaries that encapsulate the main topics, purpose, and relevant keywords of each website. This instructive technique ensures that the model's output is aligned with our project's informational needs, providing a standardized format for the extracted summaries.\n",
    "\n",
    "#### Technical Enhancements and Innovations\n",
    "To improve the speed performance of the model We incorporated optimizations such as quantization by tested varying precisions (float16, bfloat16, 8-bit, 4-bit) without seeing any significant accuracy degredation. Quantization allows for adjustments to computational efficiency, allowing our model to run effectively even in resource-constrained environments like personal laptops or less powerful cloud services. We mainly focused on using GPU's for our model, however we understand that in a Production pipeline the summarization process may not have time constraints as a batched process and could be run on a CPU.\n",
    "\n",
    "#### Challenges and Limitations\n",
    "Despite our approach, our summarization process is still computationally intensive and requires substantial resources to execute efficiently. We were able to reliably run our process on Google Colab's free T4 tier, however run close to the resource limits during long runs. Additionally, the quality and relevance of the generated summaries can vary based on the size of our input data and the model's inherent biases and limitations, necessitating review of the output for consistency and accuracy. If this were a critical or customer facing application, we would need to thoroughly ensure that the summaries are accurate.\n",
    "\n",
    "#### Future Directions and Improvements\n",
    "Looking ahead, we aim to integrate headless browser solutions like Playwright or Selenium, enabling our system to execute and interpret JavaScript, thereby capturing the full range of content available on modern websites. Additionally, refining our model's ability to discern and disregard irrelevant data during preprocessing could further streamline our summarization process, reducing computational overhead and improving the relevacy of our summaries.\n",
    "\n",
    "Some other ideas for future improvements include:\n",
    "* Using a more powerful model, or API based inferences from much larger or private models for generated summaries.\n",
    "* Flagging or adding additional context to prompts based on URL or subdomain, to add context to the summary.\n",
    "* Re-archetecturing our implementation to allow for easier switching between models, or to allow for multiple models to be run in parallel or subsequentally to compare results programmatically.\n",
    "* Tree-based summarization/ data structure, where we summarize based on levels of sub-domain and path. This would allow for a more structured and organized summary, and would allow for more complex queries/filters later during our semantic search.\n",
    "\n",
    "In summary, our project's LLM Summary Generation phase represents a significant step in automating content comprehension and summarization, offering a scalable solution to process and distill vast quantities of web data into actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\python312\\lib\\site-packages (0.43.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (0.27.2)\n",
      "Requirement already satisfied: torch in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from bitsandbytes) (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (0.21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch->bitsandbytes) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\python312\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (2.2.1+cu121)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryan\\appdata\\roaming\\python\\python312\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d4237772234696b1e9340f8b93f19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLM Model \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from textwrap import dedent\n",
    "import json\n",
    "from IPython.display import display\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Environment setup\n",
    "def is_running_on_colab():\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        return True\n",
    "    except ModuleNotFoundError:\n",
    "        return False\n",
    "\n",
    "if is_running_on_colab():\n",
    "    access_token = userdata.get('HUGGINGFACE_TOKEN')\n",
    "else:\n",
    "    access_token = access_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "MODEL = \"google/gemma-2b-it\" # Newer and small model, promising given the size to performance as well as open source\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, torch_dtype=torch.float16, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config)\n",
    "\n",
    "LLM_INSTRUCT_PROMPT = dedent('''\\\n",
    "Given the URL \"{url}\", the title of the webpage \"{title}\", and the following text from the site: \"{text}\" provide a concise summary that includes:\n",
    "1. The main topics in the text.\n",
    "2. The purpose or objective of the website, inferred from the title, text and url (including subdomain and path).\n",
    "3. Tags or keywords that a user may search to try and find the site in a search engine.\n",
    "\n",
    "You must utilize information from the URL (such as the specific path and subdomain) to contextualize and add to the understanding of the text.\n",
    "\n",
    "Format the response in a json like the following:\n",
    "\n",
    "'summary': 'The description of url and summary of the text goes here.',\n",
    "'topics': ['topic1', 'topic2', 'topic3'],\n",
    "'tags': ['tag1', 'tag2', 'tag3']\n",
    "''')\n",
    "\n",
    "def gen_summary(url: str, title:str, text:str) -> str:\n",
    "    chat = [\n",
    "        { \"role\": \"user\", \"content\": LLM_INSTRUCT_PROMPT.format(url=url, text=text, title=title) },\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**input_ids, max_new_tokens=1000)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def extract_summary(llm_output: str):\n",
    "    model_index = llm_output.find(\"'tags': ['tag1', 'tag2', 'tag3']\\nmodel\")\n",
    "    if model_index != -1:\n",
    "        json_string = llm_output[model_index + 40:]\n",
    "\n",
    "        # Replace \",] with \"] to fix json formatting, common issue with LLM\n",
    "        json_string = json_string.replace('\",]', '\"]')\n",
    "\n",
    "        # If last character is not } then add it\n",
    "        if json_string[-1] != '}':\n",
    "            json_string += '}'\n",
    "\n",
    "        # If first character is not { then add it\n",
    "        if json_string[0] != '{':\n",
    "            json_string = '{' + json_string\n",
    "\n",
    "        try:\n",
    "            json_object = json.loads(json_string)\n",
    "            return json_object\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        raise ValueError('Could not find model output in LLM output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Clrear GPU cache as it builds up\n",
    "def flush_gpu_cache():\n",
    "    if device == \"cuda:0\":\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most recent URL: https://interdisciplinarystudies.charlotte.edu, Extract: The Office of Interdisciplinary Studies promotes interdiscip...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Successes: 450, Failures: 5'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average time: 4.584820302327476, Last time: 5.368001222610474'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate Summaries\n",
    "current_display = display('Starting...', display_id=True)\n",
    "progress_display = display('Starting...', display_id=True)\n",
    "time_display = display('Starting...', display_id=True)\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "avg_time = 0\n",
    "last_time = 0\n",
    "current_time = 0\n",
    "\n",
    "# Test llm summaries\n",
    "summary_outputs = []\n",
    "extract_summaries = []\n",
    "final_urls = []\n",
    "\n",
    "test_set = list(pd.read_csv('./data/subdomain_test_set.csv').itertuples(index=False, name=None))\n",
    "\n",
    "for t in test_set:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        sum = gen_summary(t[0],t[1],t[2])\n",
    "        summary_outputs.append(sum)\n",
    "        extract = extract_summary(sum)\n",
    "        extract_summaries.append(extract)\n",
    "        if extract is not None:\n",
    "            extract['url']=t[0]\n",
    "            extract['title']=t[1]\n",
    "            final_urls.append(extract)\n",
    "            success_count += 1\n",
    "            current_display.update(f'Most recent URL: {t[0]}, Extract: {extract[\"summary\"][:60]}...')\n",
    "            progress_display.update(f'Successes: {success_count}, Failures: {failure_count}')\n",
    "\n",
    "            # Save the final urls to a json file every 10 summaries, in case of failure or runtime error. This ensures we don't waste progress\n",
    "            if success_count % 10 == 0:\n",
    "                flush_gpu_cache()\n",
    "                with open('./data/sublist_llm_summaries_from_pipeline.json', 'w') as f:\n",
    "                    json.dump(final_urls, f, indent=4)\n",
    "        else:\n",
    "            raise Exception('Error parsing json')\n",
    "    except Exception as e:\n",
    "        failure_count += 1\n",
    "        current_display.update(f'Most recent URL: {t[0]} \\nError: {e}')\n",
    "        progress_display.update(f'Successes: {success_count}, Failures: {failure_count}')\n",
    "        continue\n",
    "    end_time = time.time()\n",
    "    last_time = end_time - start_time\n",
    "    avg_time = (avg_time * (success_count - 1) + last_time) / success_count\n",
    "    time_display.update(f'Average time: {avg_time}, Last time: {last_time}')\n",
    "\n",
    "# Save the final urls to a json file one last time at the end\n",
    "with open('./data/sublist_llm_summaries_from_pipeline.json', 'w') as f:\n",
    "    json.dump(final_urls, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear llm model and objects from memory once complete\n",
    "\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Analysis\n",
    "\n",
    "In the semantic analysis phase of our project, we aimed to generate embeddings for the text data extracted from the University's website. These embeddings capture the semantic meaning of the text and enable our search engine to understand and interpret user queries contextually, thereby enhancing the relevance and accuracy of search results.\n",
    "\n",
    "### Embeddings Creation\n",
    "We utilized the Txtai library, and specifically the `txtai.Embeddings` module with the pre-trained model `sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco`. This model transforms the textual content from each web page into dense vector representations, capturing the underlying meanings and relationships within the data. The embedding process converts the summaries, topics, and tags extracted from each page into a format suitable for semantic search, thereby enhancing the relevance and precision of our search engine. There are many different models and methods for creating embeddings, and we chose this one because it was easy to use and had good performance for our needs.\n",
    "\n",
    "### Data Preparation\n",
    "Prior to embedding, we prepared the text data by combining URLs, summaries, topics, and tags into a single, displayable text format. This consolidation was necessary to ensure that the model could consider all relevant information when generating embeddings. Additionally, we implemented error handling to manage missing files.\n",
    "\n",
    "### Indexing and Search Capability\n",
    "Once the embeddings were generated, they were indexed to enable efficient semantic search across our dataset. The `embeddings.index()` function was utilized to associate each web page's URL with its corresponding embedding, laying the groundwork for our search functionality. \n",
    "\n",
    "We developed two primary functions for interacting with the indexed data:\n",
    "- `search(query, max_results=5)`: This function allows users to perform semantic searches within our dataset. By inputting a query, the function retrieves the most semantically relevant web pages, demonstrating the practical application of our embeddings in real-world search scenarios.\n",
    "- `explain(query, max_results=5)`: In addition to the standard search functionality, we implemented an 'explain' feature. This function provides insight into the reasons behind the search results, offering users transparency and a deeper understanding of the search engineâ€™s behavior.\n",
    "\n",
    "### Technical Methods\n",
    "Our implementation relies on the robust capabilities of the txtai library. The underlying technology is not new, but txtai provides a simple and effective interface for generating embeddings and performing semantic searches. During our initial research we explored how the library works by utilizing [Sentence-Transformers](https://www.sbert.net) to create and store our own embeddings in a in-memory database. In a production use case we would likely use a more powerful and scalable database, such as Elasticsearch, to store and search our embeddings and have more control over the configuration.\n",
    "\n",
    "### Limitations\n",
    "While our semantic analysis and embedding generation have significantly enhanced our project's capabilities, there are limitations. The process is heavily dependent on the quality and relevance of the initial text data. Moreover, as the embeddings are based on pre-trained models, there may be constraints related to the specificity and domain relevance of these models. Additionally, the embedding model will need to be able to run on command to embed user queries, so the size of the model is a consideration for where and how the server will be hosted. \n",
    "\n",
    "### Future Improvements\n",
    "Looking ahead, we aim to refine our embedding generation by experimenting with different models and tuning parameters to better suit our specific dataset and use cases. Additionally, enhancing our error handling and data preparation methods will improve the robustness and effectiveness of our semantic analysis.\n",
    "\n",
    "We also believe there exists an oportunity for extremely cost effective semantic search capabilities by using [Transformers.js](https://huggingface.co/docs/transformers.js/en/index), which would allow for the embedding generation and search to be done client side, and would allow for a more scalable and cost effective solution. This would also allow for more complex queries and filters to be run client side, and would allow for a more interactive and dynamic search experience. More testing of the limitations and capabilities of this for large datasets would be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/sublist_llm_summaries_from_pipeline.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 14\u001b[0m         final_urls \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "import txtai\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# Create embeddings for each web page\n",
    "embeddings = txtai.Embeddings(path=\"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\", content=True)\n",
    "\n",
    "def to_displayable_text(row):\n",
    "    return f\"{row['url']}\\n{row['title']}\\n{row['summary']}\\n{row['topics']}\\n{row['tags']}\"\n",
    "\n",
    "if 'final_urls' not in globals():\n",
    "    try:\n",
    "        with open('./data/sublist_llm_summaries_from_pipeline.json', 'r') as f:\n",
    "            final_urls = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print('No file found')\n",
    "\n",
    "if final_urls is not None:\n",
    "    embeddings.index([(row[\"url\"], to_displayable_text(row)) for row in final_urls])\n",
    "\n",
    "def search(query, max_results=1):\n",
    "    # Search index\n",
    "    results = embeddings.search(query, max_results)\n",
    "    return results\n",
    "\n",
    "def explain(query, max_results=1):\n",
    "    # Explain index\n",
    "    results = embeddings.explain(query,max_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'https://sis.charlotte.edu', 'text': \"https://sis.charlotte.edu\\nDepartment of Software information Systems - College of Computing and Informatics\\nThe Department of Software Information Systems (SIS) is a pioneer in Information Technology research and education. We offer a wide selection of courses in Information Technology and Software Engineering, with an emphasis on designing and deploying IT infrastructures that deliver integrated, secure, reliable, and easy-to-use services. We also partner with the Computer Science and Geography and Earth Sciences departments to offer specific concentrations in those fields to our students.\\n['Information Technology', 'Software Engineering', 'Computer Science', 'Geography', 'Earth Sciences']\\n['SIS', 'Department of Software information Systems', 'Information Technology', 'Software Engineering', 'Computer Science', 'Geography', 'Earth Sciences']\", 'score': 0.7893373370170593}\n",
      "{'id': 'https://dsi.charlotte.edu', 'text': \"https://dsi.charlotte.edu\\nSchool of Data Science â€“ Undergraduate Programs\\nThe School of Data Science program at the University of North Carolina at Charlotte offers an undergraduate program designed to equip students with the skills and knowledge to thrive in the data science field. The program focuses on providing a comprehensive understanding of data science principles and technologies, including machine learning, data analysis, statistics, and data visualization. Students will also gain a deep understanding of the ethical considerations surrounding data science.\\n['Data Science', 'Data Analysis', 'Statistics', 'Ethics']\\n['data science', 'data analytics', 'statistics', 'ethics']\", 'score': 0.7993032336235046}\n",
      "{'id': 'https://dsi.charlotte.edu', 'text': \"https://dsi.charlotte.edu\\nSchool of Data Science â€“ Undergraduate Programs\\nThe School of Data Science program at the University of North Carolina at Charlotte offers an undergraduate program designed to equip students with the skills and knowledge to thrive in the data science field. The program focuses on providing a comprehensive understanding of data science principles and technologies, including machine learning, data analysis, statistics, and data visualization. Students will also gain a deep understanding of the ethical considerations surrounding data science.\\n['Data Science', 'Data Analysis', 'Statistics', 'Ethics']\\n['data science', 'data analytics', 'statistics', 'ethics']\", 'score': 0.7993032336235046}\n",
      "{'id': 'https://library.charlotte.edu', 'text': \"https://library.charlotte.edu\\nHomepage | J. Murrey Atkins Library\\nThe website provides a concise overview of the J. Murrey Atkins Library, highlighting its purpose as a resource for students and researchers at UNC Charlotte. It offers access to various databases, journals, and other materials, including books, articles, and multimedia resources.\\n['Research', 'Libraries', 'Academic Resources']\\n['J. Murrey Atkins Library', 'Research', 'Academic Resources']\", 'score': 0.7625248432159424}\n",
      "{'id': 'http://vpa.charlotte.edu', 'text': \"http://vpa.charlotte.edu\\nVolunteer Program Assessment | Home\\nThe Volunteer Program Assessment (VPA) is a cutting-edge and innovative volunteer assessment system designed to promote nonprofit organizational effectiveness by focusing on the volunteer program as seen through the perspective of volunteers.\\n['Volunteer Assessment', 'Nonprofit Organizations', 'Volunteer Engagement']\\n['VPA', 'Volunteer Survey', 'Nonprofit Impact']\", 'score': 0.8016149997711182}\n",
      "Exiting the program.\n"
     ]
    }
   ],
   "source": [
    "# Python Search Example\n",
    "while True:\n",
    "    user_input = input(\"Enter command ('your search query here' or 'quit' to exit): \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "    elif len(user_input) < 5:\n",
    "        print(\"Invalid input. Please input a query longer than 4 characters.\")\n",
    "    else:\n",
    "        results = search(user_input)\n",
    "        for result in results:\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine\n",
    "\n",
    "In the search engine phase of our project, we developed a small custom front-end web application to communicate with the user and demonstrate the capabilities of our semantic search engine. The application allows users to input queries and receive relevant search results, showcasing the practical application of our semantic analysis and embedding generation.\n",
    "\n",
    "## API Integration and Functionality\n",
    "\n",
    "In the development phase of our project, we integrated two primary functionalities into our web service: `search` and `explain`, both critical to our semantic analysis system. Utilizing the Flask web framework, renowned for its ease of use and flexibility, we crafted a lightweight, RESTful API service designed to interface seamlessly with our backend logic and data embeddings.\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "We developed two endpoints within our Flask application: `/search` and `/explain`, corresponding to our core functionalities. These endpoints accept POST requests, aligning with standard API practices for data transmission and retrieval, ensuring both security and efficiency.\n",
    "\n",
    "1. **Search Endpoint**:\n",
    "    - This endpoint facilitates querying the embeddings index to find the most relevant information based on user input. It decodes the JSON payload from client requests to extract the 'query' and 'max_results' parameters, providing a user-friendly interface for data querying.\n",
    "    - Upon receiving a query, it interacts with the `search` function, which utilizes our pre-processed and indexed data, returning a list of results sorted by relevance. This process encapsulates the essence of semantic search by leveraging natural language understanding to fetch pertinent information.\n",
    "\n",
    "2. **Explain Endpoint**:\n",
    "    - Similar to the search functionality but with an added layer of interpretability, this endpoint provides insights into the query results. It helps users understand why certain pieces of information were deemed relevant by detailing the matching process.\n",
    "    - This transparency is crucial for applications requiring a deeper understanding of AI-driven decisions, enhancing user trust and system accountability.\n",
    "\n",
    "### API Design and User Experience\n",
    "\n",
    "Our API design prioritizes simplicity and efficiency, making it accessible to developers and end-users alike. By employing the Flask framework, we benefited from its minimalistic yet powerful features, enabling rapid development and deployment. The choice of POST methods for both endpoints aligns with best practices for APIs, ensuring data encapsulation and enhancing security.\n",
    "\n",
    "### Testing and Debugging\n",
    "\n",
    "The Flask application is configured to run in debug mode, facilitating real-time feedback and immediate error logging during the development process. This feature accelerates debugging and streamlines the testing phase, allowing for a more agile development workflow.\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "As the project evolves, we anticipate extending the API's capabilities, incorporating additional endpoints, and refining existing ones to accommodate a broader range of queries and use cases. Continuous integration of user feedback and performance metrics will guide these enhancements, ensuring the API remains robust, user-centric, and aligned with evolving project goals.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The creation of our Flask-based API represents a significant milestone in our project, bridging the gap between complex backend algorithms and user-facing applications. It exemplifies a successful integration of semantic technologies with web services, paving the way for advanced search and analysis tools accessible through straightforward, RESTful interfaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "[2024-03-20 17:45:26,864] ERROR in app: Exception on /search [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ryan\\AppData\\Local\\Temp\\ipykernel_6248\\1726753683.py\", line 11, in search_endpoint\n",
      "    results = search(query, max_results=max_results)\n",
      "              ^^^^^^\n",
      "NameError: name 'search' is not defined\n",
      "127.0.0.1 - - [20/Mar/2024 17:45:26] \"POST /search HTTP/1.1\" 500 -\n",
      "[2024-03-20 17:45:30,666] ERROR in app: Exception on /search [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\flask\\app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ryan\\AppData\\Local\\Temp\\ipykernel_6248\\1726753683.py\", line 11, in search_endpoint\n",
      "    results = search(query, max_results=max_results)\n",
      "              ^^^^^^\n",
      "NameError: name 'search' is not defined\n",
      "127.0.0.1 - - [20/Mar/2024 17:45:30] \"POST /search HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search_endpoint():\n",
    "    \"\"\"Search the embeddings index.\"\"\"\n",
    "    data = request.get_json()\n",
    "    query = data.get('query', '')\n",
    "    max_results = data.get('max_results', 5)\n",
    "    results = search(query, max_results=max_results)\n",
    "    return jsonify({'results': results})\n",
    "\n",
    "@app.route('/explain', methods=['POST'])\n",
    "def explain_endpoint():\n",
    "    \"\"\"Explain the embeddings index.\"\"\"\n",
    "    data = request.get_json()\n",
    "    query = data.get('query', '')\n",
    "    max_results = data.get('max_results', 5)\n",
    "    results = explain(query, max_results=max_results)\n",
    "    return jsonify({'results': results})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()  # Enable reloader and debugger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "In the evaluation phase of our project, we aimed to assess the performance and effectiveness of our semantic search engine, focusing on the relevance and accuracy of search results. We utilized a custom evaluation framework to measure the engine's performance against a manually curated test set, comparing the engine's output with the [Google Search Console powered University website search](https://search.charlotte.edu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
